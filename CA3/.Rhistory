# ---- Task_2 ----
post <- function(theta, y, X) {
eta <- X %*% theta # Logistic regression likelihood
likelihood <- prod(plogis(eta)^y * (1 - plogis(eta))^(1 - y))
prior <- exp(-0.5 * sum(theta^2 / 100)) # a priori with theta ~ N(0, 100 * I)
posterior <- likelihood * prior # posteriori is proportional to a priori * likelihood
return(posterior)
}
# ---- Task_3 ----
mh_algo <- function(theta_estimate, y, X) {
N <- 10000
theta <- matrix(nrow = N, ncol = 4)
theta[1,] <- theta_estimate # initial value
sigma <- c(0.1, 0.001) # step length
for (i in 2:N) {
theta_star <- theta[i-1,] + rnorm(4) * sigma
# check if the acceptance probability is greater than the acceptance ratio
posterior_theta_star <- post(theta_star, y, X)
posterior_theta <- post(theta[i-1,], y, X)
ratio <- runif(1)
if (posterior_theta_star/ posterior_theta > ratio) {
theta[i,] <- theta_star  # Accept the proposal
} else {
theta[i,] <- theta[i-1,]  # Reject the proposal
}
}
return(theta[i,])
}
theta_sample <- mh_algo(theta_estimate, y, X)
# Call functions built in the assignment 1
source("../Codes/Functions.R")
#########################  ASSIGNMENT_3  #########################
# Load Data
load("../proj_data.Rdata")
modell <- glm(Resultat ~ Alder + Kon + Utbildare,
data = data_individ,
family = "binomial")
summary(modell)
# Assign values for X and y
y <- matrix(data_individ$Resultat, ncol = 1)
X <- model.matrix(Resultat ~ Alder + Kon + Utbildare,
data = data_individ)
# ---- Task_1 ----
# Compute AIC = 2k - 2l(theta_ml)
n <- nrow(X)
theta0 = rep(0, ncol(X))
k <- length(theta0)
theta_estimate <- NR(theta0, 3, y, X)
log_likelihood <- l(theta_estimate, y, X)
aic_computed <- 2*k - 2*log_likelihood
#AIC output from R summary
r_summary_aic <- summary(modell)$aic
# Compute k_cv =sum(l_i(theta_i))/n
# Here, theta_i = theta_ml(X_-i)
nk_cv <- 0
for(i in 1:n) {
X_minus_i <- X[-i, , drop=FALSE]
y_minus_i <- y[-i, , drop=FALSE]
X_i <- X[i, , drop=FALSE]
y_i <- y[i, , drop=FALSE]
theta_i <- NR(theta0, 3, y_minus_i, X_minus_i)
# log likelihood for i-th observation
log_likelihood_theta_i <- l(theta_i, y_i, X_i)
nk_cv <- nk_cv + log_likelihood_theta_i
}
k_cv <- nk_cv/n
# Creating a comparison data frame
comparison_aic_values <- data.frame(
"AIC_R_model" = r_summary_aic,
"AIC_computed" = aic_computed,
"2*nK_CV_computed" = 2*nk_cv
)
# ---- Task_2 ----
post <- function(theta, y, X) {
eta <- X %*% theta # Logistic regression likelihood
likelihood <- prod(plogis(eta)^y * (1 - plogis(eta))^(1 - y))
prior <- exp(-0.5 * sum(theta^2 / 100)) # a priori with theta ~ N(0, 100 * I)
posterior <- likelihood * prior # posteriori is proportional to a priori * likelihood
return(posterior)
}
# ---- Task_3 ----
mh_algo <- function(theta_estimate, y, X) {
N <- 10000
theta <- matrix(nrow = N, ncol = 4)
theta[1,] <- theta_estimate # initial value
sigma <- c(0.1, 0.001) # step length
for (i in 2:N) {
theta_star <- theta[i-1,] + rnorm(4) * sigma
# check if the acceptance probability is greater than the acceptance ratio
posterior_theta_star <- post(theta_star, y, X)
posterior_theta <- post(theta[i-1,], y, X)
ratio <- runif(1)
if (posterior_theta_star/ posterior_theta > ratio) {
theta[i,] <- theta_star  # Accept the proposal
} else {
theta[i,] <- theta[i-1,]  # Reject the proposal
}
}
return(theta)
}
theta_sample <- mh_algo(theta_estimate, y, X)
View(theta_sample)
# Call functions built in the assignment 1
source("../Codes/Functions.R")
#########################  ASSIGNMENT_3  #########################
# Load Data
load("../proj_data.Rdata")
modell <- glm(Resultat ~ Alder + Kon + Utbildare,
data = data_individ,
family = "binomial")
summary(modell)
# Assign values for X and y
y <- matrix(data_individ$Resultat, ncol = 1)
X <- model.matrix(Resultat ~ Alder + Kon + Utbildare,
data = data_individ)
# ---- Task_1 ----
# Compute AIC = 2k - 2l(theta_ml)
n <- nrow(X)
theta0 = rep(0, ncol(X))
k <- length(theta0)
theta_estimate <- NR(theta0, 3, y, X)
log_likelihood <- l(theta_estimate, y, X)
aic_computed <- 2*k - 2*log_likelihood
#AIC output from R summary
r_summary_aic <- summary(modell)$aic
# Compute k_cv =sum(l_i(theta_i))/n
# Here, theta_i = theta_ml(X_-i)
nk_cv <- 0
for(i in 1:n) {
X_minus_i <- X[-i, , drop=FALSE]
y_minus_i <- y[-i, , drop=FALSE]
X_i <- X[i, , drop=FALSE]
y_i <- y[i, , drop=FALSE]
theta_i <- NR(theta0, 3, y_minus_i, X_minus_i)
# log likelihood for i-th observation
log_likelihood_theta_i <- l(theta_i, y_i, X_i)
nk_cv <- nk_cv + log_likelihood_theta_i
}
k_cv <- nk_cv/n
# Creating a comparison data frame
comparison_aic_values <- data.frame(
"AIC_R_model" = r_summary_aic,
"AIC_computed" = aic_computed,
"2*nK_CV_computed" = 2*nk_cv
)
# ---- Task_2 ----
post <- function(theta, y, X) {
eta <- X %*% theta # Logistic regression likelihood
likelihood <- prod(plogis(eta)^y * (1 - plogis(eta))^(1 - y))
prior <- exp(-0.5 * sum(theta^2 / 100)) # a priori with theta ~ N(0, 100 * I)
posterior <- likelihood * prior # posteriori is proportional to a priori * likelihood
return(posterior)
}
# ---- Task_3 ----
mh_algo <- function(theta_estimate, y, X) {
N <- 10000
theta <- matrix(nrow = N, ncol = 4)
theta[1,] <- theta_estimate # initial value
sigma <- standard_error(theta_estimate) # suggested sigma as standard error
for (i in 2:N) {
theta_star <- theta[i-1,] + rnorm(4) * sigma
# check if the acceptance probability is greater than the acceptance ratio
posterior_theta_star <- post(theta_star, y, X)
posterior_theta <- post(theta[i-1,], y, X)
ratio <- runif(1)
if (posterior_theta_star/ posterior_theta > ratio) {
theta[i,] <- theta_star  # Accept the proposal
} else {
theta[i,] <- theta[i-1,]  # Reject the proposal
}
}
return(theta)
}
theta_sample <- mh_algo(theta_estimate, y, X)
# Call functions built in the assignment 1
source("../Codes/Functions.R")
#########################  ASSIGNMENT_3  #########################
# Load Data
load("../proj_data.Rdata")
modell <- glm(Resultat ~ Alder + Kon + Utbildare,
data = data_individ,
family = "binomial")
summary(modell)
# Assign values for X and y
y <- matrix(data_individ$Resultat, ncol = 1)
X <- model.matrix(Resultat ~ Alder + Kon + Utbildare,
data = data_individ)
# ---- Task_1 ----
# Compute AIC = 2k - 2l(theta_ml)
n <- nrow(X)
theta0 = rep(0, ncol(X))
k <- length(theta0)
theta_estimate <- NR(theta0, 3, y, X)
log_likelihood <- l(theta_estimate, y, X)
aic_computed <- 2*k - 2*log_likelihood
#AIC output from R summary
r_summary_aic <- summary(modell)$aic
# Compute k_cv =sum(l_i(theta_i))/n
# Here, theta_i = theta_ml(X_-i)
nk_cv <- 0
for(i in 1:n) {
X_minus_i <- X[-i, , drop=FALSE]
y_minus_i <- y[-i, , drop=FALSE]
X_i <- X[i, , drop=FALSE]
y_i <- y[i, , drop=FALSE]
theta_i <- NR(theta0, 3, y_minus_i, X_minus_i)
# log likelihood for i-th observation
log_likelihood_theta_i <- l(theta_i, y_i, X_i)
nk_cv <- nk_cv + log_likelihood_theta_i
}
k_cv <- nk_cv/n
# Creating a comparison data frame
comparison_aic_values <- data.frame(
"AIC_R_model" = r_summary_aic,
"AIC_computed" = aic_computed,
"2*nK_CV_computed" = 2*nk_cv
)
# ---- Task_2 ----
post <- function(theta, y, X) {
eta <- X %*% theta # Logistic regression likelihood
likelihood <- prod(plogis(eta)^y * (1 - plogis(eta))^(1 - y))
prior <- exp(-0.5 * sum(theta^2 / 100)) # a priori with theta ~ N(0, 100 * I)
posterior <- likelihood * prior # posteriori is proportional to a priori * likelihood
return(posterior)
}
# ---- Task_3 ----
mh_algo <- function(theta_estimate, y, X) {
N <- 10000
theta <- matrix(nrow = N, ncol = 4)
theta[1,] <- theta_estimate # initial value
sigma <- standard_error(theta_estimate, y, X) # suggested sigma as standard error
for (i in 2:N) {
theta_star <- theta[i-1,] + rnorm(4) * sigma
# check if the acceptance probability is greater than the acceptance ratio
posterior_theta_star <- post(theta_star, y, X)
posterior_theta <- post(theta[i-1,], y, X)
ratio <- runif(1)
if (posterior_theta_star/ posterior_theta > ratio) {
theta[i,] <- theta_star  # Accept the proposal
} else {
theta[i,] <- theta[i-1,]  # Reject the proposal
}
}
return(theta)
}
theta_sample <- mh_algo(theta_estimate, y, X)
plot(x = c(1:10000), y = theta_sample[,1], type = "l", col = "black", main="theta1", xlab= "iteration", ylab = "theta1")
plot(x = c(1:10000), y = theta_sample[,2], type = "l", col = "black", main="theta2", xlab= "iteration", ylab = "theta2")
plot(x = c(1:10000), y = theta_sample[,3], type = "l", col = "black", main="theta3", xlab= "iteration", ylab = "theta3")
plot(x = c(1:10000), y = theta_sample[,4], type = "l", col = "black", main="theta4", xlab= "iteration", ylab = "theta4")
knitr::opts_chunk$set(echo = TRUE)
par(mfrow=c(2,2))
plot(x = c(1:10000), y = theta_sample[,1], type = "l", col = "black",
main="theta1 Parameter Plot", xlab= "#iterations", ylab = "theta1")
plot(x = c(1:10000), y = theta_sample[,2], type = "l", col = "black",
main="theta2 Parameter Plot", xlab= "#iterations", ylab = "theta2")
plot(x = c(1:10000), y = theta_sample[,3], type = "l", col = "black",
main="theta3 Parameter Plot", xlab= "#iterations", ylab = "theta3")
plot(x = c(1:10000), y = theta_sample[,4], type = "l", col = "black",
main="theta4 Parameter Plot", xlab= "#iterations", ylab = "theta4")
par(mfrow=c(2,2))
hist(x = theta_sample[(1:10000), 1], col = "black",
main="theta1 Posteriors Histograms", xlab= "#iterations", ylab = "theta1")
hist(x = theta_sample[(1:10000), 2],  col = "black",
main="theta2 Posteriors Histograms", xlab= "#iterations", ylab = "theta2")
hist(x = theta_sample[(1:10000), 3], col = "black",
main="theta3 Posteriors Histograms", xlab= "#iterations", ylab = "theta3")
hist(x = theta_sample[(1:10000), 4], col = "black",
main="theta4 Posteriors Histograms", xlab= "#iterations", ylab = "theta4")
par(mfrow=c(2,2))
hist(x = theta_sample[(1:10000), 1], col = "lightgray",
main="theta1 Posteriors Histograms", xlab= "#iterations", ylab = "theta1")
hist(x = theta_sample[(1:10000), 2],  col = "lightgray",
main="theta2 Posteriors Histograms", xlab= "#iterations", ylab = "theta2")
hist(x = theta_sample[(1:10000), 3], col = "lightgray",
main="theta3 Posteriors Histograms", xlab= "#iterations", ylab = "theta3")
hist(x = theta_sample[(1:10000), 4], col = "lightgray",
main="theta4 Posteriors Histograms", xlab= "#iterations", ylab = "theta4")
#remove the first 100 iterations
nburnin <- 100
thetasamples <- matrix(nrow = 9900, ncol = 4)
thetasamples <- theta[(nburnin+1):10000, ]
---
title: "Part III: Bayesian statistics and some model choice."
#remove the first 100 iterations
nburnin <- 100
thetasamples <- matrix(nrow = 9900, ncol = 4)
thetasamples <- theta[(nburnin+1):10000, ]
iter <- 10000
theta <- matrix(nrow = iter, ncol = 4)
# starting point: thetaML
theta[1,] <- thetaML #summary(model)$coefficients[, 1]
thetaML <- summary(model)$coefficients[, 1]
y <- matrix(data_individ$Resultat, ncol = 1)
X <- model.matrix(Resultat ~ Alder + Kon + Utbildare,
data = data_individ)
model <- glm(Resultat ~ Alder + Kon + Utbildare, data = data_individ, family = "binomial")
summary(model)
y <- matrix(data_individ$Resultat, ncol = 1)
X <- model.matrix(Resultat ~ Alder + Kon + Utbildare,
data = data_individ)
thetaML <- summary(model)$coefficients[, 1]
se <- summary(model)$coefficients[, 2]
# initialize variables
K_CV <- 0
thetaMLi <- c()
yi <- c()
Xi <- matrix(nrow = 1, ncol = 4)
for(i in 1:nrow(X)){
# maximum likelihood values for the dataset without the i-th participant
thetaMLi <- summary(glm(Resultat ~ Alder + Kon + Utbildare, data = data_individ[-i,], family = "binomial"))$coefficients[, 1]
# explanatory variables for the i-th participant
yi <- y[i]
Xi[1,] <- X[i,]
# sum the log likelihoods
K_CV <- K_CV + l(thetaMLi, yi, Xi)
}
# divide by number of participants
K_CV <- K_CV/nrow(X)
# K_CV <- 0
# Xi <- matrix(nrow = 1, ncol = 4)
# for(i in 1:nrow(X)){
#   Xi[1,] <- X[i,]
#   K_CV <- K_CV + l(thetaML, y[i], Xi)
# }
Xtest <- cbind(1, 18:25, rep(c(0, 1), 4), rep(c(1, 1, 0, 0), 2))
ytest <- c(rep(TRUE, 4), rep(FALSE, 4))
post(c(260, -10, 10, -20), ytest, Xtest) / post(c(270, -15, 15, -25), ytest , Xtest)
iter <- 10000
theta <- matrix(nrow = iter, ncol = 4)
# starting point: thetaML
theta[1,] <- thetaML #summary(model)$coefficients[, 1]
# standard errors
sigma <- se #summary(model)$coefficients[, 2]
for (i in 2:iter){
# each theta*[i]step n ~ N(theta[i] step n-1 , standard error from model^2)
theta.star <- theta[i-1,] + rnorm(4) * sigma # not sigma^2! sigma here gives a N( , sigma^2)
# Metropolis algorithm iterations
if (post(theta.star, y, X) / post(theta[i-1,], y, X) > runif(1)){
theta[i,] <- theta.star
}else{
theta[i,] <- theta[i-1,]
}
}
plot(x = c(1:10000), y = theta[,1], type = "l", col = "black", main="theta1", xlab= "iteration", ylab = "theta1")
plot(x = c(1:10000), y = theta[,2], type = "l", col = "black", main="theta2", xlab= "iteration", ylab = "theta2")
plot(x = c(1:10000), y = theta[,3], type = "l", col = "black", main="theta3", xlab= "iteration", ylab = "theta3")
plot(x = c(1:10000), y = theta[,4], type = "l", col = "black", main="theta4", xlab= "iteration", ylab = "theta4")
#remove the first 100 iterations
nburnin <- 100
thetasamples <- matrix(nrow = 9900, ncol = 4)
thetasamples <- theta[(nburnin+1):10000, ]
hist(thetasamples[,1], breaks = 30, main = "Posterior (non normalized) theta1", xlab = "theta1", ylab = "Count")
hist(thetasamples[,2], breaks = 30, main = "Posterior (non normalized) theta2", xlab = "theta2", ylab = "Count")
hist(thetasamples[,3], breaks = 30, main = "Posterior (non normalized) theta3", xlab = "theta3", ylab = "Count")
hist(thetasamples[,4], breaks = 30, main = "Posterior (non normalized) theta4", xlab = "theta4", ylab = "Count")
View(theta_sample)
par(mfrow=c(2,2))
hist(x = theta_sample[(1:10000), 1], col = "lightgray",
main="theta1 Posterior Histogram", xlab= "theta1", ylab = "frequency")
hist(x = theta_sample[(1:10000), 2],  col = "lightgray",
main="theta2 Posterior Histogram", xlab= "theta2", ylab = "frequency")
hist(x = theta_sample[(1:10000), 3], col = "lightgray",
main="theta3 Posterior Histogram", xlab= "theta3", ylab = "frequency")
hist(x = theta_sample[(1:10000), 4], col = "lightgray",
main="theta4 Posterior Histogram", xlab= "theta4", ylab = "frequency")
for(i in 1:4){
density <- density(thetasamples[,i])
plot(density, main = paste("Posterior ditribution theta", i, sep = ""))
}
View(y)
View(X)
View(X)
View(X)
View(X)
# a privately educated subject of your own sex and age.
# intercept = 1
# Alder = 26    # Own age
# KonMan = 0    # Female
# Utbildare = 0 # Privately educated
X_star <- c(1, 26, 0, 0)
# model the probability using regression framework
p_x <- function(theta, x) {
return (1 / (1 + exp(-t(theta)*x)))
}
prob_y <- c()
for(i in 1:10000) {
theta <- theta_sample[i, ]
prob_y[i] <- p_x(theta, X_star)
}
print(prob_y)
# a privately educated subject of your own sex and age.
# intercept = 1
# Alder = 26    # Own age
# KonMan = 0    # Female
# Utbildare = 0 # Privately educated
X_star <- c(1, 26, 0, 0)
# model the probability using regression framework
p_x <- function(theta, x) {
return (1 / (1 + exp(-t(theta)*x)))
}
prob_y <- c()
for(i in 1:10000) {
theta <- theta_sample[i, ]
prob_y[i] <- p_x(theta, X_star)
}
hist(prob_y)
# a privately educated subject of your own sex and age.
# intercept = 1
# Alder = 26    # Own age
# KonMan = 0    # Female
# Utbildare = 0 # Privately educated
X_star <- c(1, 26, 0, 0)
# model the probability using regression framework
p_x <- function(theta, x) {
return (1 / (1 + exp(-t(theta)*x)))
}
prob_y <- c(10000)
for(i in 1:10000) {
theta <- theta_sample[i, ]
prob_y[i] <- p_x(theta, X_star)
}
hist(prob_y)
expected_prob_y <- mean(prob_y)
# a privately educated subject of your own sex and age.
# intercept = 1
# Alder = 26    # Own age
# KonMan = 0    # Female
# Utbildare = 0 # Privately educated
X_star <- c(1, 26, 0, 0)
# model the probability using regression framework
p_x <- function(theta, x) {
return (1 / (1 + exp(-t(theta)*x)))
}
prob_y <- c(10000)
for(i in 1:10000) {
theta <- theta_sample[i, ]
prob_y[i] <- p_x(theta, X_star)
}
hist(prob_y)
expected_prob_y <- mean(prob_y)
# a privately educated subject of your own sex and age.
# intercept = 1
# Alder = 26    # Own age
# KonMan = 0    # Female
# Utbildare = 0 # Privately educated
X_star <- c(1, 26, 0, 0)
# model the probability using regression framework
p_x <- function(theta, x) {
return (1 / (1 + exp(-t(theta)*x)))
}
prob_y <- c(10000)
for(i in 1:10000) {
theta <- theta_sample[i, ]
prob_y[i] <- p_x(theta, X_star)
}
hist(prob_y)
expected_prob_y <- mean(prob_y)
# a privately educated subject of your own sex and age.
# intercept = 1
# Alder = 26    # Own age
# KonMan = 0    # Female
# Utbildare = 0 # Privately educated
X_star <- c(1, 26, 0, 0)
# model the probability using regression framework
p_x <- function(theta, x) {
return (1 / (1 + exp(-t(theta)*x)))
}
prob_y <- rep(0, 10000)
for(i in 1:10000) {
theta <- theta_sample[i, ]
prob_y[i] <- p_x(theta, X_star)
}
hist(prob_y)
expected_prob_y <- mean(prob_y)
Implemented the Metropolis-Hastings algorithm over 10000 iterations while
# a privately educated subject of your own sex and age.
# intercept = 1
# Alder = 26    # Own age
# KonMan = 0    # Female
# Utbildare = 0 # Privately educated
X_star <- c(1, 26, 0, 0)
# model the probability using regression framework
p_x <- function(theta, x) {
return (1 / (1 + exp(-t(theta)*x)))
}
# compute the probability of pass wrt X_star
prob_y <- rep(0, 10000)
for(i in 1:10000) {
theta <- theta_sample[i, ]
prob_y[i] <- p_x(theta, X_star)
}
expected_prob_y <- mean(prob_y)
print("expected probability of y* : ", expected_prob_y)
# a privately educated subject of your own sex and age.
# intercept = 1
# Alder = 26    # Own age
# KonMan = 0    # Female
# Utbildare = 0 # Privately educated
X_star <- c(1, 26, 0, 0)
# model the probability using regression framework
p_x <- function(theta, x) {
return (1 / (1 + exp(-t(theta)*x)))
}
# compute the probability of pass wrt X_star
prob_y <- rep(0, 10000)
for(i in 1:10000) {
theta <- theta_sample[i, ]
prob_y[i] <- p_x(theta, X_star)
}
expected_prob_y <- mean(prob_y)
cat("expected probability of y* : ", expected_prob_y)
