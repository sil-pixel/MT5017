---
title: "Assignment Part III"
author: "Silpa Soni Nallacheruvu (19980824-5287) Hernan(20000526-4999)"
date: "`r Sys.Date()`"
output: pdf_document
---

set.seed(980824)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE}
source("../Codes/Functions.R")
source("codes_assignment_3.R")
```

# Summary

# Task 1

## Approach :

To calculate the AIC, the formula

$2*k - 2*l(\hat{\theta_{ml}})$

was applied where k is the number of parameters in the theta. To calculate 
the leave-one-out cross-validation, the formula

$\frac{\sum_{i=1}^{n} {l_i(\hat{theta_i)}}}{n}$

was applied where $\hat{\theta_i}$ is $\hat{\theta_{ml}}(X_{-i})$,
$(X_{-i})$ is one observation $(X_{i})$ is left out of X, $l_i(\hat{theta_i)}$ 
is the log likelihood for the i-th left out observation and 
n is the total number of observations. 
These values were then compared with the AIC from R in the provided summary.

## Code :

```{r echo=TRUE}
# ---- Task_1 ----

# Compute AIC = 2k - 2l(theta_ml)
n <- nrow(X)
theta0 = rep(0, ncol(X))
k <- length(theta0)
theta_estimate <- NR(theta0, 3, y, X) 
log_likelihood <- l(theta_estimate, y, X)
aic_computed <- 2*k - 2*log_likelihood

#AIC output from R summary 
r_summary_aic <- summary(modell)$aic

# Compute k_cv =sum(l_i(theta_i))/n
# Here, theta_i = theta_ml(X_-i)
nk_cv <- 0
for(i in 1:n) {
  X_minus_i <- X[-i, , drop=FALSE]
  y_minus_i <- y[-i, , drop=FALSE]
  X_i <- X[i, , drop=FALSE]
  y_i <- y[i, , drop=FALSE]
  theta_i <- NR(theta0, 3, y_minus_i, X_minus_i)
  # log likelihood for i-th observation
  log_likelihood_theta_i <- l(theta_i, y_i, X_i)
  nk_cv <- nk_cv + log_likelihood_theta_i
}

k_cv <- nk_cv/n

# Creating a comparison data frame 
comparison_aic_values <- data.frame(
  "AIC_R_model" = r_summary_aic,
  "AIC_computed" = aic_computed,
  "2*nK_CV_computed" = 2*nk_cv
)
```

## Output :

```{r}
comparison_aic_values
```

## Observation :

The computed AIC value and the AIC value from the R summary coincide in value.
The computed K_CV is in the same magnitude as $AIC/(-2n)$ as expected. 

# Task 2

## Approach :

The a posteriori density combines the a priori and the likelihood of the data. For the logistic regression, we have y data, X and the parameter vector $_theta$. so the a posteriori density is proportional to:
$P(\theta|y,X) \propto P(y|X,\theta)P(\theta) $
where $P(\theta)$ is the a prior density and $P(y1X,\theta)$ is. the likelihood from the logistic regression model.

for a binary outcome, the likelihood for the logistic regression is given by the following equation:

$P(y_{i}|X_{i},\theta) =  \frac{1}{(1+\exp(-X_{i}\theta)}$

The a priori is Gaussian: $\theta N(0, 100I)$ so the a prior density is the following:
$P(\theta) \propto exp(-\frac{1}{2}\theta^T(100I)^{-1}\theta)$

## Code :

```{r echo=TRUE}
# ---- Task_2 ----
post <- function(theta, y, X) {
  eta <- X %*% theta # Logistic regression likelihood
  likelihood <- prod(plogis(eta)^y * (1 - plogis(eta))^(1 - y))
  prior <- exp(-0.5 * sum(theta^2 / 100)) # a priori with theta ~ N(0, 100 * I)
  posterior <- likelihood * prior # posteriori is proportional to a priori * likelihood
  return(posterior)
}

```

## Output :

```{r}
Xtest <- cbind(1, 18:25, rep(c(0, 1), 4), rep(c(1, 1, 0, 0), 2))
ytest <- c(rep(TRUE, 4), rep(FALSE, 4))
testing<-post(c(260, -10, 10, -20), ytest, Xtest) / post(c(270, -15, 15, -25), ytest , Xtest)
testing
```

## Observation :
Given the results obtained by the test the function works correctly.
# Task 3

## Approach :

## Code :

```{r echo=TRUE}
# ---- Task_3 ----


```

## Output :

```{r}

```

## Observation :
